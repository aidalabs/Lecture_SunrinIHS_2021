{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tensorflow**"
      ],
      "metadata": {
        "id": "JYYsLGXsVNst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Tensorflow 개요**"
      ],
      "metadata": {
        "id": "eiGm9JmFWdhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. Tensorflow란?"
      ],
      "metadata": {
        "id": "cDYDQENVWq_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Google에서 개발한 딥러닝과 기계 학습을 위한 오픈소스 라이브러리\n",
        "- 2015년에 공개하여 지금까지 계속 업데이트 및 지원이 진행 중임"
      ],
      "metadata": {
        "id": "aGSK7cEIWu-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. Tensorflow의 기능적 소개\n",
        "- 그래프 형태로 수치 연산을 하는 라이브러리\n",
        "- '텐서(tensor)'라는 데이터 구조를 사용함\n",
        "- 텐서\n",
        "    - 수학적으로는 쉽게 말하면 다차원 배열을 나타냄\n",
        "    - TensorFlow에서는 이러한 텐서를 노드(node)라고 부르는 연산들 사이를 흐르는 데이터로 사용함\n",
        "- 즉, TensorFlow는 데이터의 흐름을 그래프로 표현하고, 이 그래프를 계산하여 딥러닝 모델을 학습하고 실행하는 도구\n",
        "    - 그래프는 연산(operation)들과 변수(variable)들로 구성됨\n",
        "    - 변수들은 학습 중에 업데이트되는 모델의 파라미터를 의미함"
      ],
      "metadata": {
        "id": "7MZnVGVUZOLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3. TensorFlow의 장점\n",
        "- 딥러닝 모델을 구성하고, 학습시키며, 추론(inference)하는 작업을 보다 간단하고 유연하게 만들 수 있음\n",
        "- GPU나 TPU와 같은 가속기를 활용하여 계산을 빠르게 처리할 수 있기 때문에, 대용량 데이터와 복잡한 모델에도 적용하기 용이함\n",
        "- 쉽고 다양한 API를 제공하므로 사용자가 원하는 수준에서 모델을 구성할 수 있음\n",
        "- 이미 구현된 많은 딥러닝 모델과 레이어를 포함하여 고수준의 추상화된 기능을 제공함\n",
        "- 자유롭게 커스터마이징이 가능함\n",
        "- 다른 라이브러리나 프레임워크와 쉽게 통합하여 사용할 수 있음"
      ],
      "metadata": {
        "id": "jxmKUpL4ZPJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-4. Tensorflow의 사용 현황\n",
        "- TensorFlow는 딥러닝과 머신러닝의 다양한 분야에서 널리 사용되고 있으며, 컴퓨터 비전, 자연어 처리, 음성 인식 등 다양한 애플리케이션에 적용되고 있음\n",
        "- 활발한 커뮤니티가 많으며, 이러한 커뮤니티를 통해 지속적인 지원과 발전이 이루어지고 있음"
      ],
      "metadata": {
        "id": "ToO9XRdbZO8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Tensorflow를 사용하는 과정**"
      ],
      "metadata": {
        "id": "26L1nXg0WKK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 필요한 패키지를 가져온다.\n",
        "- 텐서를 만들고 사용한다.\n",
        "- GPU 가속을 사용한다.\n",
        "- tf.data.Dataset로 데이터 파이프라인을 구축한다."
      ],
      "metadata": {
        "id": "7hVZzi_oak5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. 필요한 패키지(Tensorflow) 가져오기"
      ],
      "metadata": {
        "id": "gOnU1_Kjaxck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2NHJ-737UlAA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. Tensor 만들고 사용하기"
      ],
      "metadata": {
        "id": "RdJjPkwsVJtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tensor\n",
        "    - 다차원 배열\n",
        "    - NumPy ndarray 객체와 유사하게 tf.Tensor 객체에도 dtype(데이터 유형)과 shape(형상)가 있음\n",
        "    - tf.Tensor는 가속기 메모리(예: GPU)에 상주할 수 있음\n",
        "    - tf.math.add, tf.linalg.matmul, tf.linalg.inv 등 다양한 연산 라이브러리 지원\n",
        "    - 연산 라이브러리는 기본 Python 유형을 자동으로 변환하여 사용/적용 가능"
      ],
      "metadata": {
        "id": "F2iUMEFZEJXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.math.add(1, 2))\n",
        "print(tf.math.add([1, 2], [3, 4]))\n",
        "print(tf.math.square(5))\n",
        "print(tf.math.reduce_sum([1, 2, 3]))\n",
        "\n",
        "# Operator overloading is also supported\n",
        "print(tf.math.square(2) + tf.math.square(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC18KMiSUsEL",
        "outputId": "68af1982-b360-4860-ae4c-91475935f6ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor([4 6], shape=(2,), dtype=int32)\n",
            "tf.Tensor(25, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(13, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.linalg.matmul([[1]], [[2, 3]])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj5ovL-0UsBS",
        "outputId": "e625535e-69ff-4617-9c94-f0b480da569f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n",
            "(1, 2)\n",
            "<dtype: 'int32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Numpy와의 호환성\n",
        "    - 간단한 TensorFlow tf.Tensor와 NumPy ndarray 사이의 변환\n",
        "        - 텐서플로 연산은 자동으로 NumPy 배열을 텐서로 변환함\n",
        "        - NumPy 연산은 자동으로 텐서를 NumPy 배열로 변환함\n",
        "    - 텐서는 .numpy() 메서드(method)를 호출하여 NumPy 배열로 변환할 수 있음\n",
        "        - tf.Tensor와 배열은 메모리 표현을 공유하기 때문에 이러한 변환은 일반적으로는 간단함\n",
        "    - 텐서와 NumPy 배열의 차이\n",
        "        - tf.Tensor는 GPU 메모리에 저장될 수 있고, NumPy 배열은 항상 호스트 메모리에 저장됨\n",
        "        - 따라서 이러한 변환이 항상 가능한 것은 아님\n",
        "        - GPU 메모리와 호스트 메모리에 각각 저장된 배열을 변환하려면 GPU에서 호스트 메모리로 복사하는 작업이 필요함"
      ],
      "metadata": {
        "id": "So2_3_IRFJR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "ndarray = np.ones([3, 3])\n",
        "\n",
        "print(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\n",
        "tensor = tf.math.multiply(ndarray, 42)\n",
        "print(tensor)\n",
        "\n",
        "\n",
        "print(\"And NumPy operations convert Tensors to NumPy arrays automatically\")\n",
        "print(np.add(tensor, 1))\n",
        "\n",
        "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
        "print(tensor.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSPIkmS8Ur-U",
        "outputId": "37bd4676-c334-4171-bc25-ff6d83273cbe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow operations convert numpy arrays to Tensors automatically\n",
            "tf.Tensor(\n",
            "[[42. 42. 42.]\n",
            " [42. 42. 42.]\n",
            " [42. 42. 42.]], shape=(3, 3), dtype=float64)\n",
            "And NumPy operations convert Tensors to NumPy arrays automatically\n",
            "[[43. 43. 43.]\n",
            " [43. 43. 43.]\n",
            " [43. 43. 43.]]\n",
            "The .numpy() method explicitly converts a Tensor to a numpy array\n",
            "[[42. 42. 42.]\n",
            " [42. 42. 42.]\n",
            " [42. 42. 42.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-3. GPU 가속 사용하기"
      ],
      "metadata": {
        "id": "SJxDzl7QJcq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GPU 가속의 사용\n",
        "    - 대부분의 TensorFlow 연산은 GPU를 사용하여 가속화됨\n",
        "    - 어떠한 코드를 명시하지 않아도, TensorFlow는 연산을 위해 CPU 또는 GPU를 사용할 것인지를 자동으로 결정함\n",
        "    - 필요 시 텐서를 CPU와 GPU 메모리 사이에서 복사\n",
        "    - 연산에 의해 생성된 텐서는 전형적으로 연산이 실행된 장치의 메모리에 의해 실행됨"
      ],
      "metadata": {
        "id": "4E9Uri6OGULE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.uniform([3, 3])\n",
        "\n",
        "print(\"Is there a GPU available: \"),\n",
        "print(tf.config.list_physical_devices(\"GPU\"))\n",
        "\n",
        "print(\"Is the Tensor on GPU #0:  \"),\n",
        "print(x.device.endswith('GPU:0'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs4JNrzcUr7b",
        "outputId": "1cda560b-d6cd-42c5-a352-d65e73e6ab0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a GPU available: \n",
            "[]\n",
            "Is the Tensor on GPU #0:  \n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 장치 명\n",
        "    - Tensor.device는 텐서를 구성하고 있는 호스트 장치의 풀네임을 제공함\n",
        "    - 제공되는 호스트 장치 명\n",
        "        - 프로그램이 실행중인 호스트의 네트워크 주소 및 해당 호스트 내의 장치와 같은 많은 세부 정보를 인코딩\n",
        "        - 텐서플로 프로그램의 분산 실행에 필요함\n",
        "        - 텐서가 호스트의 N번째 GPU에 놓여지면 문자열은 GPU:<N>으로 끝남\n",
        "    - 명시적 장치 배치\n",
        "        - TensorFlow에서 배치란 개별 연산이 실행을 위해 장치에 할당(배치)되는 방식을 나타냄\n",
        "        - 명시적인 지침이 없으면 TensorFlow는 연산을 실행할 장치를 자동으로 결정하고 필요한 경우 해당 장치에 텐서를 복사함\n",
        "        - 명시적으로 배치하려면 tf.device 컨텍스트 관리자를 사용하여 특정 장치에 배치 가능"
      ],
      "metadata": {
        "id": "t5hqhcTiH1D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def time_matmul(x):\n",
        "  start = time.time()\n",
        "  for loop in range(10):\n",
        "    tf.linalg.matmul(x, x)\n",
        "\n",
        "  result = time.time()-start\n",
        "\n",
        "  print(\"10 loops: {:0.2f}ms\".format(1000*result))\n",
        "\n",
        "# Force execution on CPU\n",
        "print(\"On CPU:\")\n",
        "with tf.device(\"CPU:0\"):\n",
        "  x = tf.random.uniform([1000, 1000])\n",
        "  assert x.device.endswith(\"CPU:0\")\n",
        "  time_matmul(x)\n",
        "\n",
        "# Force execution on GPU #0 if available\n",
        "if tf.config.list_physical_devices(\"GPU\"):\n",
        "  print(\"On GPU:\")\n",
        "  with tf.device(\"GPU:0\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\n",
        "    x = tf.random.uniform([1000, 1000])\n",
        "    assert x.device.endswith(\"GPU:0\")\n",
        "    time_matmul(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRCcUTAfUr4u",
        "outputId": "3d35cef4-f071-490a-9cdf-f2a4ccf7b808"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On CPU:\n",
            "10 loops: 327.01ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-4. tf.data.Dataset로 데이터 파이프라인을 구축하기"
      ],
      "metadata": {
        "id": "CeEmH9cQJsDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 세트의 사용\n",
        "    - tf.data.Dataset API를 사용하여 모델에 데이터를 공급하기 위한 파이프라인을 구축\n",
        "    - tf.data.Dataset\n",
        "        - 모델의 훈련 또는 평가 루프에 데이터를 제공할 단순하고 재사용 가능한 부분으로부터 성능이 뛰어나고 복잡한 입력 파이프라인을 구축하는 데 사용됨"
      ],
      "metadata": {
        "id": "dA8AUvj8Iq8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 소스 데이터세트 생성하기\n",
        "    1. tf.data.Dataset.from_tensors, tf.data.Dataset.from_tensor_slices 등의 팩토리 함수 중 하나를 사용\n",
        "    2. tf.data.TextLineDataset 또는 tf.data.TFRecordDataset와 같은 파일에서 읽는 객체를 사용"
      ],
      "metadata": {
        "id": "TuKKMQMiJ1Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tensors = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Create a CSV file\n",
        "import tempfile\n",
        "_, filename = tempfile.mkstemp()\n",
        "\n",
        "with open(filename, 'w') as f:\n",
        "  f.write(\"\"\"Line 1\n",
        "Line 2\n",
        "Line 3\n",
        "  \"\"\")\n",
        "\n",
        "ds_file = tf.data.TextLineDataset(filename)"
      ],
      "metadata": {
        "id": "QaK_KyZ4Ur14"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터세트 변환 적용\n",
        "    - tf.data.Dataset.map, tf.data.Dataset.batch 및 tf.data.Dataset.shuffle과 같은 변환 함수를 사용하여 데이터세트 레코드에 변환 적용"
      ],
      "metadata": {
        "id": "DPfQ5dadJVb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tensors = ds_tensors.map(tf.math.square).shuffle(2).batch(2)\n",
        "\n",
        "ds_file = ds_file.batch(2)"
      ],
      "metadata": {
        "id": "xhVpYiGgUrzJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 반복하기\n",
        "    - tf.data.Dataset는 레코드 순회를 지원하는 반복 가능한 객체"
      ],
      "metadata": {
        "id": "k_P0MpgFKGKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Elements of ds_tensors:')\n",
        "for x in ds_tensors:\n",
        "  print(x)\n",
        "\n",
        "print('\\nElements in ds_file:')\n",
        "for x in ds_file:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH1yWi3LUrwQ",
        "outputId": "6036204b-e5ad-4288-9275-26f3f21cd440"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elements of ds_tensors:\n",
            "tf.Tensor([4 9], shape=(2,), dtype=int32)\n",
            "tf.Tensor([16  1], shape=(2,), dtype=int32)\n",
            "tf.Tensor([25 36], shape=(2,), dtype=int32)\n",
            "\n",
            "Elements in ds_file:\n",
            "tf.Tensor([b'Line 1' b'Line 2'], shape=(2,), dtype=string)\n",
            "tf.Tensor([b'Line 3' b'  '], shape=(2,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. 정형 데이터 다루기**"
      ],
      "metadata": {
        "id": "hSk0Laj1VPeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1. 정형 데이터를 다루는 기본적인 방법"
      ],
      "metadata": {
        "id": "1Ci1-wW7Vh0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 실습 내용\n",
        "    - 판다스(Pandas)를 사용하여 CSV 파일을 읽기\n",
        "    - tf.data를 사용하여 행을 섞고 배치로 나누는 입력 파이프라인(pipeline)을 만들기\n",
        "    - CSV의 열을 feature_column을 사용해 모델 훈련에 필요한 특성으로 매핑하기\n",
        "- 데이터 셋\n",
        "    - 클리블랜드(Cleveland) 심장병 재단에서 제공한 작은 데이터셋을 사용함\n",
        "    - CSV 파일은 수백 개의 행으로 이루어져 있음\n",
        "    - 각 행은 환자 한 명을 나타내고 각 열은 환자에 대한 속성 값을 나타냄\n",
        "    - 데이터를 사용해 환자의 심장병 발병 여부를 예측(이진 분류)하기 위한 모델에 적용할 수 있도록 매핑"
      ],
      "metadata": {
        "id": "kxR4u9IpVs7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. 데이터 처리"
      ],
      "metadata": {
        "id": "a5LUuE6VWwCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 필요한 라이브러리 import"
      ],
      "metadata": {
        "id": "iF2MUgdhW4dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnFUvQIIVT_3",
        "outputId": "f9c5e59a-dda4-4b4b-d76c-45104447c9c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "M14iY_hUWtpQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pandas로 DataFrame 만들기"
      ],
      "metadata": {
        "id": "fWIYBiyFW7kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = pd.read_csv('https://raw.githubusercontent.com/aidalabs/Lectures/master/LectureFiles/dataset/Heart.csv')\n",
        "dataframe = pd.read_csv(URL)\n",
        "dataframe.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "BvCpcT_gW-dY",
        "outputId": "24719896-9529-47c0-fe5c-d32e5123d51b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-bbbf4ab1b496>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://storage.googleapis.com/applied-dl/heart.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터를 훈련 세트(Training Set), 검증 세트(Validation Set), 테스트 세트(Test Set)로 분할하기"
      ],
      "metadata": {
        "id": "3jSaD-7wXBq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), '훈련 샘플')\n",
        "print(len(val), '검증 샘플')\n",
        "print(len(test), '테스트 샘플')"
      ],
      "metadata": {
        "id": "0hZdZHm9XMVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tf.data를 사용하여 입력 파이프라인 만들기\n",
        "    - tf.data를 사용하여 데이터프레임을 감싸면 특성 열을 사용하여 판다스 데이터프레임의 열을 모델 훈련에 필요한 특성으로 매핑할 수 있음"
      ],
      "metadata": {
        "id": "-WpC6WHUXOzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 판다스 데이터프레임으로부터 tf.data 데이터셋을 만들기 위한 함수\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "wCkj5WpOXRkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단하게 출력하기 위해 작은 배치 크기를 사용함\n",
        "batch_size = 5\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "UuP3VndTXjGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 입력 파이프라인 이해하기\n",
        "    - 앞에서 만든 입력 파이프라인을 호출하여 반환되는 데이터 포맷을 확인함\n",
        "    - 이 데이터셋은 (DataFrame의) 열 이름을 키로 갖는 딕셔너리를 반환함\n",
        "    - DataFrame 열의 값이 매핑되어 있음"
      ],
      "metadata": {
        "id": "lvAnA1R4XkQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  print('전체 특성:', list(feature_batch.keys()))\n",
        "  print('나이 특성의 배치:', feature_batch['age'])\n",
        "  print('타깃의 배치:', label_batch )"
      ],
      "metadata": {
        "id": "Wt7cyTZ5Xl87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-3. 여러 종류의 특성 열 알아 보기"
      ],
      "metadata": {
        "id": "7Y7trmjjYC-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 몇 가지 특성 열을 만들어서 데이터프레임의 열을 변환하는 방법을 확인함"
      ],
      "metadata": {
        "id": "BH8c2hexYJc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성 열을 시험해 보기 위해 샘플 배치를 만듭니다.\n",
        "example_batch = next(iter(train_ds))[0]"
      ],
      "metadata": {
        "id": "Gher2AQMYGM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성 열을 만들고 배치 데이터를 변환하는 함수\n",
        "def demo(feature_column):\n",
        "  feature_layer = layers.DenseFeatures(feature_column)\n",
        "  print(feature_layer(example_batch).numpy())"
      ],
      "metadata": {
        "id": "1ZPFng8QYPkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 수치형 열(Numerical Column)\n",
        "    - 가장 간단한 종류의 열\n",
        "    - 실수 특성을 표현하는데 사용\n",
        "    - 이 열을 사용하면 모델은 데이터프레임 열의 값을 변형시키지 않고 그대로 전달 받음\n",
        "    - 특성 열의 출력은 모델의 입력이 됨\n",
        "    - 심장병 데이터셋 데이터프레임의 대부분 열은 수치형임"
      ],
      "metadata": {
        "id": "dxUzbEcjYPDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age = feature_column.numeric_column(\"age\")\n",
        "demo(age)"
      ],
      "metadata": {
        "id": "HqooXaYXYtZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 버킷형 열(Bucketized Column)\n",
        "\t- 종종 모델에 수치 값을 바로 주입하기 원치 않을 때, 수치 값의 구간을 나누어 이를 기반으로 범주형으로 변환\n",
        "\t- 이때 수치 데이터를 수치형 열로 표현하는 대신 버킷형 열을 사용하여 몇 개의 버킷(bucket)으로 분할할 수 있음\n",
        "\t- 분할 후, 원-핫 인코딩(one-hot encoding)된 값은 각 열이 매칭되는 나이 범위를 나타냄\n"
      ],
      "metadata": {
        "id": "aK76FhgLZ3et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "demo(age_buckets)"
      ],
      "metadata": {
        "id": "EcRx0solZ49g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 범주형 열(Categorical Column)\n",
        "\t- 이 데이터셋에서 thal 열은 문자열(예: 'fixed', 'normal', 'reversible')\n",
        "\t- 모델에 문자열을 바로 주입할 수 없으므로 문자열을 먼저 수치형으로 매핑해야 함\n",
        "\t- 범주형 열을 사용하여 문자열을 원-핫 벡터로 표현\n",
        "\t- 문자열 목록은 categorical_column_with_vocabulary_list를 사용하여 리스트로 전달하거나 categorical_column_with_vocabulary_file을 사용하여 파일에서 읽을 수 있음"
      ],
      "metadata": {
        "id": "A84iQVyQYwjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thal = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'thal', ['fixed', 'normal', 'reversible'])\n",
        "\n",
        "thal_one_hot = feature_column.indicator_column(thal)\n",
        "demo(thal_one_hot)"
      ],
      "metadata": {
        "id": "mPeLXqhXZTvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 임베딩 열(Embedding Column)\n",
        "    - 가능한 문자열이 몇 개가 있는 것이 아니라 범주마다 수천 개 이상의 값이 있는 경우, 여러 가지 이유로 범주의 개수가 늘어남에 따라 원-핫 인코딩을 사용하여 신경망을 훈련시키는 것이 불가능해 질 수 있음(임베딩 열을 사용하면 이런 제한을 극복할 수 있음)\n",
        "    - 고차원 원-핫 벡터로 데이터를 표현하는 대신 임베딩 열을 사용하여 저차원으로 데이터를 표현함\n",
        "    - 이 벡터는 0 또는 1이 아니라 각 원소에 어떤 숫자도 넣을 수 있는 밀집 벡터(dense vector) 임\n",
        "    - 임베딩의 크기는 튜닝 대상 파라미터\n",
        "    - 범주형 열에 가능한 값이 많을 때는 임베딩 열을 사용하는 것이 최선이라고 할 수 있음"
      ],
      "metadata": {
        "id": "MODYlRYRaPsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 열의 입력은 앞서 만든 범주형 열입니다.\n",
        "thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
        "demo(thal_embedding)"
      ],
      "metadata": {
        "id": "wyumafE6ayUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 해시 특성 열\n",
        "    - 가능한 값이 많은 범주형 열을 표현하는 또 다른 방법은 categorical_column_with_hash_bucket을 사용하는 것\n",
        "    - 이 특성 열은 입력의 해시(hash) 값을 계산한 다음 hash_bucket_size 크기의 버킷 중 하나를 선택하여 문자열을 인코딩함\n",
        "    - 이 열을 사용할 때는 어휘 목록을 제공할 필요가 없고 공간을 절약하기 위해 실제 범주의 개수보다 훨씬 작게 해시 버킷(bucket)의 크기를 정할 수 있음\n",
        "    - 이 기법의 큰 단점은 다른 문자열이 같은 버킷에 매핑될 수 있다는 것이지만 실전에서는 대체로 잘 작동함"
      ],
      "metadata": {
        "id": "OTx-4jRFa-aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thal_hashed = feature_column.categorical_column_with_hash_bucket(\n",
        "      'thal', hash_bucket_size=1000)\n",
        "demo(feature_column.indicator_column(thal_hashed))"
      ],
      "metadata": {
        "id": "uTXsf4HKdAtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 교차 특성 열(Feature Cross Column)\n",
        "    - 여러 특성을 연결하여 하나의 특성으로 만드는 것\n",
        "    - 모델이 특성의 조합에 대한 가중치를 학습할 수 있음\n",
        "    - crossed_column은 모든 가능한 조합에 대한 해시 테이블을 만들지 않고 hashed_column 매개변수를 사용하여 해시 테이블의 크기를 선택함"
      ],
      "metadata": {
        "id": "nEHckcOydGNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
        "demo(feature_column.indicator_column(crossed_feature))"
      ],
      "metadata": {
        "id": "Rca-CPbzdXo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-4. 사용할 열 선택하기"
      ],
      "metadata": {
        "id": "49BZO-5Mdv9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 제대로 된 모델을 만들어야 한다면 대용량의 데이터셋을 사용하고 어떤 특성을 포함하는 것이 가장 의미있는지, 또 어떻게 표현해야 할지 신중하게 생각하여야 함"
      ],
      "metadata": {
        "id": "cxu5Vg64daVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns = []\n",
        "\n",
        "# 수치형 열\n",
        "for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\n",
        "  feature_columns.append(feature_column.numeric_column(header))\n",
        "\n",
        "# 버킷형 열\n",
        "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "feature_columns.append(age_buckets)\n",
        "\n",
        "# 범주형 열\n",
        "thal = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'thal', ['fixed', 'normal', 'reversible'])\n",
        "thal_one_hot = feature_column.indicator_column(thal)\n",
        "feature_columns.append(thal_one_hot)\n",
        "\n",
        "# 임베딩 열\n",
        "thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
        "feature_columns.append(thal_embedding)\n",
        "\n",
        "# 교차 특성 열\n",
        "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
        "crossed_feature = feature_column.indicator_column(crossed_feature)\n",
        "feature_columns.append(crossed_feature)"
      ],
      "metadata": {
        "id": "glamec88dlpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-5. 특성 층 만들기"
      ],
      "metadata": {
        "id": "xtHimdNld1O3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 특성 열을 정의하고 나면 DenseFeatures 층을 사용해 케라스 모델에 주입할 수 있음"
      ],
      "metadata": {
        "id": "Hjj2zhiddqbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "metadata": {
        "id": "ssUMOKvgd6Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞에서는 특성 열의 작동 예를 보이기 위해 작은 배치 크기를 사용했으나\n",
        "# 여기에서는 조금 더 큰 배치 크기로 입력 파이프라인을 만듦\n",
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "dZC0fQSvd_-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. 모델 생성, 컴파일, 훈련**"
      ],
      "metadata": {
        "id": "Gxw-wI11el0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 여기서는 그냥 작동 상황만 확인함\n",
        "- 관련된 부분은 DNN 구현에서 다시 확인"
      ],
      "metadata": {
        "id": "-Syt3cfqesqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds,\n",
        "          validation_data=val_ds,\n",
        "          epochs=5)"
      ],
      "metadata": {
        "id": "JAHrN3FAeqBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"정확도\", accuracy)"
      ],
      "metadata": {
        "id": "rLHeu5uDe3Qg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}