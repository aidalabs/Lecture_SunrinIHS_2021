{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nBLXbt4fZYFW",
        "QtcP0mxlZYFd",
        "H5kqnSe8ZYFh",
        "aDhIDm6lZYFi",
        "mWwu4d06ZYFk",
        "RWlNHB7jZYFl"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN을 이용한 자연어 처리**"
      ],
      "metadata": {
        "id": "1cgBZi3LES-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "- 참고도서\n",
        "  - 파이토치로 배우는 자연어처리 (델립라오, 브라이언 맥머핸 지음 / 박해선 옮김 | 한빛미디어)\n",
        "  - 텐서플로 2와 머신러닝으로 시작하는 자연어 처리  (전창욱, 최태균, 조종현, 신성진 지음 | 위키북스)\n",
        "  - 처음 배우는 딥러닝 챗봇 (조경래 지음 | 한빛미디어)\n",
        "---"
      ],
      "metadata": {
        "id": "QoPf1E_uP99L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. RNN 구현하기**"
      ],
      "metadata": {
        "id": "OohvSC2LuadM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 PyTorch"
      ],
      "metadata": {
        "id": "VmOEmZiwvAmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "TdbVyRij01-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\", \"you like cat\", \"you love milk\", \"you hate coffee\"]\n",
        "dtype = torch.float\n",
        "sentences"
      ],
      "metadata": {
        "id": "DEUipDqK1Bvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Word Processing\n",
        "\"\"\"\n",
        "word_list = list(set(\" \".join(sentences).split()))\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "n_class = len(word_dict)"
      ],
      "metadata": {
        "id": "SD6fZ4NA1IiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list"
      ],
      "metadata": {
        "id": "H1ENT2kw1Kqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict"
      ],
      "metadata": {
        "id": "3JdWBCZ01duI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_dict"
      ],
      "metadata": {
        "id": "UpNectQa1irb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_class"
      ],
      "metadata": {
        "id": "rQb6N7EN1owz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TextRNN Parameter\n",
        "\"\"\"\n",
        "batch_size = len(sentences)\n",
        "n_step = 2  # 학습 하려고 하는 문장의 길이 - 1\n",
        "n_hidden = 5  # 은닉층 사이즈"
      ],
      "metadata": {
        "id": "BzBD2P3d1rBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(sentences):\n",
        "  input_batch = []\n",
        "  target_batch = []\n",
        "\n",
        "  for sen in sentences:\n",
        "    word = sen.split()\n",
        "    input = [word_dict[n] for n in word[:-1]]\n",
        "    target = word_dict[word[-1]]\n",
        "\n",
        "    input_batch.append(np.eye(n_class)[input])  # One-Hot Encoding\n",
        "    target_batch.append(target)\n",
        "\n",
        "  return input_batch, target_batch"
      ],
      "metadata": {
        "id": "1mixRKqJ13cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch, target_batch = make_batch(sentences)\n",
        "input_batch = torch.tensor(input_batch, dtype=torch.float32, requires_grad=True)\n",
        "target_batch = torch.tensor(target_batch, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "wOc23iEg2Tf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TextRNN\n",
        "\"\"\"\n",
        "class TextRNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TextRNN, self).__init__()\n",
        "\n",
        "    self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n",
        "    self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
        "    self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
        "    self.Softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, hidden, X):\n",
        "    X = X.transpose(0, 1)\n",
        "    outputs, hidden = self.rnn(X, hidden)\n",
        "    outputs = outputs[-1]  # 최종 예측 Hidden Layer\n",
        "    model = torch.mm(outputs, self.W) + self.b  # 최종 예측 최종 출력 층\n",
        "    return model"
      ],
      "metadata": {
        "id": "3qr2dpAU2eL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training\n",
        "\"\"\"\n",
        "model = TextRNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "  hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
        "  output = model(hidden, input_batch)\n",
        "  loss = criterion(output, target_batch)\n",
        "\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "input = [sen.split()[:2] for sen in sentences]\n",
        "\n",
        "hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
        "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
        "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
      ],
      "metadata": {
        "id": "0vSW1WaOvDTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 가장 단순하게 사용하려면..."
      ],
      "metadata": {
        "id": "06AntUanvpM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "input_size = 4\n",
        "hidden_size = 2\n",
        "\n",
        "# 1-hot encoding\n",
        "h = [1, 0, 0, 0]\n",
        "e = [0, 1, 0, 0]\n",
        "l = [0, 0, 1, 0]\n",
        "o = [0, 0, 0, 1]\n",
        "\n",
        "input_data_np = np.array([[h, e, l, l, o],\n",
        "                           [e, o, l, l, l],\n",
        "                           [l, l, e, e, l]], dtype=np.float32)\n",
        "\n",
        "# transform as torch tensor\n",
        "input_data = torch.Tensor(input_data_np)\n",
        "rnn = torch.nn.RNN(input_size, hidden_size)\n",
        "outputs, _status = rnn(input_data)"
      ],
      "metadata": {
        "id": "JlneQooOvstM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "gr3URVPFwSBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_status"
      ],
      "metadata": {
        "id": "GOaMwLYywp9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tensorflow"
      ],
      "metadata": {
        "id": "3Z7GIwFBvD8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "어휘 사이즈는 1000, 은닉 벡터 사이즈는 64로 설정\n",
        "RNN 내부 유닛은 128개, 출력 클래스의 개수는 10개로 설정\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "model.add(layers.SimpleRNN(128))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "VwPLrvkovGJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tensorflow를 이용한 sine 곡선 예측 RNN 모델"
      ],
      "metadata": {
        "id": "T9rLYEJ2IyxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, LSTM, SimpleRNN"
      ],
      "metadata": {
        "id": "uRCwX8znJBod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# time step만큼 시퀀스 데이터 분리\n",
        "def split_sequence(sequence, step):\n",
        "    x, y = list(), list()\n",
        "\n",
        "    for i in range(len(sequence)):\n",
        "        end_idx = i + step\n",
        "        if end_idx > len(sequence) - 1:\n",
        "            break\n",
        "\n",
        "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
        "        x.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "\n",
        "    return np.array(x), np.array(y)"
      ],
      "metadata": {
        "id": "aVN4xN70JBIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sin 함수 학습 데이터\n",
        "x = [i for i in np.arange(start=-10, stop=10, step=0.1)]\n",
        "train_y = [np.sin(i) for i in x]"
      ],
      "metadata": {
        "id": "n_DcxdthJGRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터\n",
        "n_timesteps = 15\n",
        "n_features = 1"
      ],
      "metadata": {
        "id": "Mb9d399GJGMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시퀀스 나누기\n",
        "# train_x.shape => (samples, timesteps)\n",
        "# train_y.shape => (samples)\n",
        "train_x, train_y = split_sequence(train_y, step=n_timesteps)\n",
        "print(\"shape x:{} / y:{}\".format(train_x.shape, train_y.shape))"
      ],
      "metadata": {
        "id": "29JNR4HuJGHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN 입력 벡터 크기를 맞추기 위해 벡터 차원 크기 변경\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], n_features)\n",
        "print(\"train_x.shape = {}\".format(train_x.shape))\n",
        "print(\"train_y.shape = {}\".format(train_y.shape))"
      ],
      "metadata": {
        "id": "po47sgmTJGAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN 모델 정의\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=10, return_sequences=False, input_shape=(n_timesteps, n_features)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "6w8v6XGKJM-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "np.random.seed(0)\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, mode='auto')\n",
        "history = model.fit(train_x, train_y, epochs=1000, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "nULYM4C6JM3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss 그래프 생성\n",
        "plt.plot(history.history['loss'], label=\"loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "khGF7MIuJMsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터셋 생성\n",
        "test_x = np.arange(10, 20, 0.1)\n",
        "calc_y = np.cos(test_x) # 테스트 정답 데이터"
      ],
      "metadata": {
        "id": "0tCNMIOTJMok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN 모델 예측 및 로그 저장\n",
        "test_y = calc_y[:n_timesteps]\n",
        "for i in range(len(test_x) - n_timesteps):\n",
        "    net_input = test_y[i : i + n_timesteps]\n",
        "    net_input = net_input.reshape((1, n_timesteps, n_features))\n",
        "    train_y = model.predict(net_input, verbose=0)\n",
        "    print(test_y.shape, train_y.shape, i, i + n_timesteps)\n",
        "    test_y = np.append(test_y, train_y)"
      ],
      "metadata": {
        "id": "IoBnR4tlJTBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 결과 그래프 그리기\n",
        "plt.plot(test_x, calc_y, label=\"ground truth\", color=\"orange\")\n",
        "plt.plot(test_x, test_y, label=\"predicitons\", color=\"blue\")\n",
        "plt.legend(loc='upper left')\n",
        "plt.ylim(-2, 2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GQuFR6cFI7QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. RNN으로 성씨 분류하기**\n",
        "\n",
        "- 이 예제는 파이토치가 제공하는 `PackedSequence`의 처리 방식을 사용하도록 변경한 것임.\n",
        "- `PackedSequence`가 편리하긴 하지만 열 인덱싱을 어떻게 처리하는지 이해하는 것은 향후에 유용함"
      ],
      "metadata": {
        "id": "DUV1ApQEDDr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 데이터 읽어오기\n",
        "- GDrive를 마운트하여 데이터 파일을 읽어올 수 있도록 준비함"
      ],
      "metadata": {
        "id": "fUxK76TgulJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "_3vBG0YpA5dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = '/gdrive/My Drive/Colab Notebooks/00_Lectures/NLP/data/'"
      ],
      "metadata": {
        "id": "5lJHZIrtl0vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 패키지 임포트"
      ],
      "metadata": {
        "id": "mPZKbo5wBSLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from argparse import Namespace"
      ],
      "metadata": {
        "id": "lumYMwVPBJS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 읽어오기 설정"
      ],
      "metadata": {
        "id": "phpQIvX9BVQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    raw_dataset_csv=\"{}{}\".format(ROOT_PATH, \"surnames/surnames.csv\"),\n",
        "    train_proportion=0.7,\n",
        "    val_proportion=0.15,\n",
        "    test_proportion=0.15,\n",
        "    output_munged_csv=\"{}{}\".format(ROOT_PATH, \"surnames/surnames_with_splits.csv\"),\n",
        "    seed=1337\n",
        ")"
      ],
      "metadata": {
        "id": "1Bb6mQSHBaLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RawData 읽어오기"
      ],
      "metadata": {
        "id": "sMtVJ6j9BstO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read raw data\n",
        "surnames = pd.read_csv(args.raw_dataset_csv, header=0)"
      ],
      "metadata": {
        "id": "7wYSjLL2Baxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "surnames.head()"
      ],
      "metadata": {
        "id": "QX9R2zt5Bv-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique classes\n",
        "set(surnames.nationality)"
      ],
      "metadata": {
        "id": "hD4osCxXByQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 국적을 기반으로 데이터 정리"
      ],
      "metadata": {
        "id": "-0sSQE-YB6bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting train by nationality\n",
        "# Create dict\n",
        "by_nationality = collections.defaultdict(list)\n",
        "for _, row in surnames.iterrows():\n",
        "    by_nationality[row.nationality].append(row.to_dict())"
      ],
      "metadata": {
        "id": "EsRsw8rHB0ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create split data\n",
        "final_list = []\n",
        "np.random.seed(args.seed)\n",
        "for _, item_list in sorted(by_nationality.items()):\n",
        "    np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_proportion*n)\n",
        "    n_val = int(args.val_proportion*n)\n",
        "    n_test = int(args.test_proportion*n)\n",
        "\n",
        "    # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'\n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)"
      ],
      "metadata": {
        "id": "cMQthPHvB5Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write split data to file\n",
        "final_surnames = pd.DataFrame(final_list)"
      ],
      "metadata": {
        "id": "uLPbfcgECBXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_surnames.split.value_counts()"
      ],
      "metadata": {
        "id": "tWhXKIKMCC5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_surnames.head()"
      ],
      "metadata": {
        "id": "fGw0yTQiCEU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write munged data to CSV\n",
        "final_surnames.to_csv(args.output_munged_csv, index=False)"
      ],
      "metadata": {
        "id": "i_TZ1fsBCGGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 패키지 임포트"
      ],
      "metadata": {
        "id": "hbzO99ctFh00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "bl35C957Kugq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 데이터 벡터 변환"
      ],
      "metadata": {
        "id": "gKYuZ0l4K0jG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vocabulary"
      ],
      "metadata": {
        "id": "xci0rRApD-7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
        "\n",
        "    def __init__(self, token_to_idx=None):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
        "        \"\"\"\n",
        "\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "\n",
        "        self._idx_to_token = {idx: token\n",
        "                              for token, idx in self._token_to_idx.items()}\n",
        "\n",
        "    def to_serializable(self):\n",
        "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
        "        return {'token_to_idx': self._token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
        "\n",
        "        매개변수:\n",
        "            token (str): Vocabulary에 추가할 토큰\n",
        "        반환값:\n",
        "            index (int): 토큰에 상응하는 정수\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_many(self, tokens):\n",
        "        \"\"\"토큰 리스트를 Vocabulary에 추가합니다.\n",
        "\n",
        "        매개변수:\n",
        "            tokens (list): 문자열 토큰 리스트\n",
        "        반환값:\n",
        "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
        "        \"\"\"\n",
        "        return [self.add_token(token) for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"토큰에 대응하는 인덱스를 추출합니다.\n",
        "\n",
        "        매개변수:\n",
        "            token (str): 찾을 토큰\n",
        "        반환값:\n",
        "            index (int): 토큰에 해당하는 인덱스\n",
        "        \"\"\"\n",
        "        return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
        "\n",
        "        매개변수:\n",
        "            index (int): 찾을 인덱스\n",
        "        반환값:\n",
        "            token (str): 인텍스에 해당하는 토큰\n",
        "        에러:\n",
        "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)"
      ],
      "metadata": {
        "id": "HOHOyj1MD1IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- SequenceVocabulary"
      ],
      "metadata": {
        "id": "EfX4UMIVEDAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self._mask_token = mask_token\n",
        "        self._unk_token = unk_token\n",
        "        self._begin_seq_token = begin_seq_token\n",
        "        self._end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self._mask_token)\n",
        "        self.unk_index = self.add_token(self._unk_token)\n",
        "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self._unk_token,\n",
        "                         'mask_token': self._mask_token,\n",
        "                         'begin_seq_token': self._begin_seq_token,\n",
        "                         'end_seq_token': self._end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
        "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
        "\n",
        "        매개변수:\n",
        "            token (str): 찾을 토큰\n",
        "        반환값:\n",
        "            index (int): 토큰에 해당하는 인덱스\n",
        "        노트:\n",
        "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
        "            `unk_index`가 0보다 커야 합니다.\n",
        "        \"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]"
      ],
      "metadata": {
        "id": "KbOylVwYD1E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vectorizer"
      ],
      "metadata": {
        "id": "8BRLqqncEKPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameVectorizer(object):\n",
        "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
        "    def __init__(self, char_vocab, nationality_vocab):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            char_vocab (Vocabulary): 문자를 정수로 매핑합니다\n",
        "            nationality_vocab (Vocabulary): 국적을 정수로 매핑합니다\n",
        "        \"\"\"\n",
        "        self.char_vocab = char_vocab\n",
        "        self.nationality_vocab = nationality_vocab\n",
        "\n",
        "    def vectorize(self, surname, vector_length=-1):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            title (str): 문자열\n",
        "            vector_length (int): 인덱스 벡터의 길이를 맞추기 위한 매개변수\n",
        "        \"\"\"\n",
        "        indices = [self.char_vocab.begin_seq_index]\n",
        "        indices.extend(self.char_vocab.lookup_token(token)\n",
        "                       for token in surname)\n",
        "        indices.append(self.char_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "        out_vector[:len(indices)] = indices\n",
        "        out_vector[len(indices):] = self.char_vocab.mask_index\n",
        "\n",
        "        return out_vector, len(indices)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, surname_df):\n",
        "        \"\"\"데이터셋 데이터프레임으로 SurnameVectorizer 객체를 초기화합니다.\n",
        "\n",
        "        매개변수:\n",
        "            surname_df (pandas.DataFrame): 성씨 데이터셋\n",
        "        반환값:\n",
        "            SurnameVectorizer 객체\n",
        "        \"\"\"\n",
        "        char_vocab = SequenceVocabulary()\n",
        "        nationality_vocab = Vocabulary()\n",
        "\n",
        "        for index, row in surname_df.iterrows():\n",
        "            for char in row.surname:\n",
        "                char_vocab.add_token(char)\n",
        "            nationality_vocab.add_token(row.nationality)\n",
        "\n",
        "        return cls(char_vocab, nationality_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
        "        nat_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
        "\n",
        "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'char_vocab': self.char_vocab.to_serializable(),\n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
      ],
      "metadata": {
        "id": "jXkybzAQD1Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DataLoader"
      ],
      "metadata": {
        "id": "6yAS_yYDEN7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameDataset(Dataset):\n",
        "    def __init__(self, surname_df, vectorizer):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            surname_df (pandas.DataFrame): 데이터셋\n",
        "            vectorizer (SurnameVectorizer): 데이터셋에서 만든 Vectorizer 객체\n",
        "        \"\"\"\n",
        "        self.surname_df = surname_df\n",
        "        self._vectorizer = vectorizer\n",
        "\n",
        "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\n",
        "\n",
        "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                             'val': (self.val_df, self.validation_size),\n",
        "                             'test': (self.test_df, self.test_size)}\n",
        "\n",
        "        self.set_split('train')\n",
        "\n",
        "        # 클래스 가중치\n",
        "        class_counts = self.train_df.nationality.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
        "        \"\"\"데이터셋을 로드하고 새로운 Vectorizer 객체를 만듭니다\n",
        "\n",
        "        매개변수:\n",
        "            surname_csv (str): 데이터셋의 위치\n",
        "        반환값:\n",
        "            SurnameDataset의 객체\n",
        "        \"\"\"\n",
        "        surname_df = pd.read_csv(surname_csv)\n",
        "        train_surname_df = surname_df[surname_df.split=='train']\n",
        "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
        "        \"\"\" 데이터셋과 새로운 Vectorizer 객체를 로드합니다.\n",
        "        캐시된 Vectorizer 객체를 재사용할 때 사용합니다.\n",
        "\n",
        "        매개변수:\n",
        "            surname_csv (str): 데이터셋의 위치\n",
        "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
        "        반환값:\n",
        "            SurnameDataset의 인스턴스\n",
        "        \"\"\"\n",
        "        surname_df = pd.read_csv(surname_csv)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(surname_df, vectorizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        \"\"\"파일에서 Vectorizer 객체를 로드하는 정적 메서드\n",
        "\n",
        "        매개변수:\n",
        "            vectorizer_filepath (str): 직렬화된 Vectorizer 객체의 위치\n",
        "        반환값:\n",
        "            SurnameVectorizer의 인스턴스\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        \"\"\"Vectorizer 객체를 json 형태로 디스크에 저장합니다\n",
        "\n",
        "        매개변수:\n",
        "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\" 벡터 변환 객체를 반환합니다 \"\"\"\n",
        "        return self._vectorizer\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
        "\n",
        "        매개변수:\n",
        "            index (int): 데이터 포인트 인덱스\n",
        "        반환값:\n",
        "            다음 값을 담고 있는 딕셔너리:\n",
        "                특성 (x_data)\n",
        "                레이블 (y_target)\n",
        "                특성 길이 (x_length)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "\n",
        "        surname_vector, vec_length = \\\n",
        "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
        "\n",
        "        nationality_index = \\\n",
        "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "\n",
        "        return {'x_data': surname_vector,\n",
        "                'y_target': nationality_index,\n",
        "                'x_length': vec_length}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
        "\n",
        "        매개변수:\n",
        "            batch_size (int)\n",
        "        반환값:\n",
        "            배치 개수\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size\n",
        "\n",
        "\n",
        "\n",
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=True, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
        "    걱 텐서를 지정된 장치로 이동합니다.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict"
      ],
      "metadata": {
        "id": "dasRPqA4D7rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 모델"
      ],
      "metadata": {
        "id": "KXdciQdAETjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.1 RNN 모델 구현"
      ],
      "metadata": {
        "id": "qpp_tZfrEZ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ElmanRNN(nn.Module):\n",
        "    \"\"\" RNNCell을 사용하여 만든 엘만 RNN \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            input_size (int): 입력 벡터 크기\n",
        "            hidden_size (int): 은닉 상태 벡터 크기\n",
        "            batch_first (bool): 0번째 차원이 배치인지 여부\n",
        "        \"\"\"\n",
        "        super(ElmanRNN, self).__init__()\n",
        "\n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def _initial_hidden(self, batch_size):\n",
        "        return torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "    def forward(self, x_in, initial_hidden=None):\n",
        "        \"\"\" ElmanRNN의 정방향 계산\n",
        "\n",
        "        매개변수:\n",
        "            x_in (torch.Tensor): 입력 데이터 텐서\n",
        "                If self.batch_first: x_in.shape = (batch_size, seq_size, feat_size)\n",
        "                Else: x_in.shape = (seq_size, batch_size, feat_size)\n",
        "            initial_hidden (torch.Tensor): RNN의 초기 은닉 상태\n",
        "        반환값:\n",
        "            hiddens (torch.Tensor): 각 타임 스텝에서 RNN 출력\n",
        "                If self.batch_first:\n",
        "                   hiddens.shape = (batch_size, seq_size, hidden_size)\n",
        "                Else: hiddens.shape = (seq_size, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            batch_size, seq_size, feat_size = x_in.size()\n",
        "            x_in = x_in.permute(1, 0, 2)\n",
        "        else:\n",
        "            seq_size, batch_size, feat_size = x_in.size()\n",
        "\n",
        "        hiddens = []\n",
        "\n",
        "        if initial_hidden is None:\n",
        "            initial_hidden = self._initial_hidden(batch_size)\n",
        "            initial_hidden = initial_hidden.to(x_in.device)\n",
        "\n",
        "        hidden_t = initial_hidden\n",
        "\n",
        "        for t in range(seq_size):\n",
        "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
        "            hiddens.append(hidden_t)\n",
        "\n",
        "        hiddens = torch.stack(hiddens)\n",
        "\n",
        "        if self.batch_first:\n",
        "            hiddens = hiddens.permute(1, 0, 2)\n",
        "\n",
        "        return hiddens"
      ],
      "metadata": {
        "id": "u-mt5UtxVLFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- column_gather\n",
        "  - SurnameClassifier 클래스(분류기 구현)에서 사용하기 위한 함수\n",
        "  - 데이터 포인트에서 마지막 벡터를 추출함"
      ],
      "metadata": {
        "id": "wVfYoYdUVhub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def column_gather(y_out, x_lengths):\n",
        "    ''' y_out에 있는 각 데이터 포인트에서 마지막 벡터 추출합니다\n",
        "\n",
        "    조금 더 구체적으로 말하면 배치 행 인덱스를 순회하면서\n",
        "    x_lengths에 있는 값에 해당하는 인덱스 위치의 벡터를 반환합니다.\n",
        "\n",
        "    매개변수:\n",
        "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
        "            shape: (batch, sequence, feature)\n",
        "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
        "            shape: (batch,)\n",
        "\n",
        "    반환값:\n",
        "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
        "            shape: (batch, feature)\n",
        "    '''\n",
        "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
        "\n",
        "    out = []\n",
        "    for batch_index, column_index in enumerate(x_lengths):\n",
        "        out.append(y_out[batch_index, column_index])\n",
        "\n",
        "    return torch.stack(out)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vbALlQsBEWUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- SurnameClassifier 클래스(분류기) 구현"
      ],
      "metadata": {
        "id": "_SxJ9h8WV_Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" RNN으로 특성을 추출하고 MLP로 분류하는 분류 모델 \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            embedding_size (int): 문자 임베딩의 크기\n",
        "            num_embeddings (int): 임베딩할 문자 개수\n",
        "            num_classes (int): 예측 벡터의 크기\n",
        "                노트: 국적 개수\n",
        "            rnn_hidden_size (int): RNN의 은닉 상태 크기\n",
        "            batch_first (bool): 입력 텐서의 0번째 차원이 배치인지 시퀀스인지 나타내는 플래그\n",
        "            padding_idx (int): 텐서 패딩을 위한 인덱스;\n",
        "                torch.nn.Embedding을 참고하세요\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                                embedding_dim=embedding_size,\n",
        "                                padding_idx=padding_idx)\n",
        "        self.rnn = ElmanRNN(input_size=embedding_size,\n",
        "                             hidden_size=rnn_hidden_size,\n",
        "                             batch_first=batch_first)\n",
        "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                         out_features=rnn_hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                          out_features=num_classes)\n",
        "\n",
        "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
        "        \"\"\" 분류기의 정방향 계산\n",
        "\n",
        "        매개변수:\n",
        "            x_in (torch.Tensor): 입력 데이터 텐서\n",
        "                x_in.shape는 (batch, input_dim)입니다\n",
        "            x_lengths (torch.Tensor): 배치에 있는 각 시퀀스의 길이\n",
        "                시퀀스의 마지막 벡터를 찾는데 사용합니다\n",
        "            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n",
        "                크로스-엔트로피 손실을 사용하려면 False로 지정합니다\n",
        "        반환값:\n",
        "            결과 텐서. tensor.shape는 (batch, output_dim)입니다.\n",
        "        \"\"\"\n",
        "        x_embedded = self.emb(x_in)\n",
        "        y_out = self.rnn(x_embedded)\n",
        "\n",
        "        if x_lengths is not None:\n",
        "            y_out = column_gather(y_out, x_lengths)\n",
        "        else:\n",
        "            y_out = y_out[:, -1, :]\n",
        "\n",
        "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
        "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        return y_out"
      ],
      "metadata": {
        "id": "OHXpJVQBVeh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 유틸리티"
      ],
      "metadata": {
        "id": "2_kJ2RbNWITz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed_everywhere(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "metadata": {
        "id": "lJnn4byvWETT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 설정"
      ],
      "metadata": {
        "id": "Hl6pty2sWM9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    # 날짜와 경로 정보\n",
        "    surname_csv=\"{}{}\".format(ROOT_PATH, \"surnames/surnames_with_splits.csv\"),\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"{}{}\".format(ROOT_PATH, \"model_storage/surname_classification\"),\n",
        "    # 모델 하이퍼파라미터\n",
        "    char_embedding_size=100,\n",
        "    rnn_hidden_size=64,\n",
        "    # 훈련 하이퍼파라미터\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    seed=1337,\n",
        "    early_stopping_criteria=5,\n",
        "    # 실행 옵션\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "# CUDA 체크\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "print(\"CUDA 사용여부: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "# 재현성을 위해 시드 설정\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# 디렉토리 처리\n",
        "handle_dirs(args.save_dir)"
      ],
      "metadata": {
        "id": "2s_xtACMWJ30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 코랩에서 사용할 데이터 다운로드\n",
        "- GDrive에 저장된 파일을 사용한다면 실행하지 않아도 무방함"
      ],
      "metadata": {
        "id": "S-TgYIR_WUBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 만약 코랩에서 실행하는 경우 아래 코드를 실행하여 전처리된 데이터를 다운로드하세요.\n",
        "!mkdir data\n",
        "!wget https://git.io/JtSPf -O data/download.py\n",
        "!wget https://git.io/JtSPU -O data/get-all-data.sh\n",
        "!chmod 755 data/get-all-data.sh\n",
        "%cd data\n",
        "!./get-all-data.sh\n",
        "%cd .."
      ],
      "metadata": {
        "id": "JfVIKtFrWOKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 로드 및 분류기 객체 생성"
      ],
      "metadata": {
        "id": "Q3Doell7XQPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
        "    # 체크포인트를 로드합니다.\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # 데이터셋과 Vectorizer를 만듭니다.\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "classifier = SurnameClassifier(embedding_size=args.char_embedding_size,\n",
        "                               num_embeddings=len(vectorizer.char_vocab),\n",
        "                               num_classes=len(vectorizer.nationality_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index)"
      ],
      "metadata": {
        "id": "OQMiONrDWfoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.2 모델 학습"
      ],
      "metadata": {
        "id": "5Poq6xVyXZCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_state(args):\n",
        "    return {'stop_early': False,\n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'learning_rate': args.learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': args.model_state_file}"
      ],
      "metadata": {
        "id": "4_6gv9kRWh7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_train_state(args, model, train_state):\n",
        "    \"\"\"훈련 상태를 업데이트합니다.\n",
        "\n",
        "    콤포넌트:\n",
        "     - 조기 종료: 과대 적합 방지\n",
        "     - 모델 체크포인트: 더 나은 모델을 저장합니다\n",
        "\n",
        "    :param args: 메인 매개변수\n",
        "    :param model: 훈련할 모델\n",
        "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
        "    :returns:\n",
        "        새로운 훈련 상태\n",
        "    \"\"\"\n",
        "\n",
        "    # 적어도 한 번 모델을 저장합니다\n",
        "    if train_state['epoch_index'] == 0:\n",
        "        torch.save(model.state_dict(), train_state['model_filename'])\n",
        "        train_state['stop_early'] = False\n",
        "\n",
        "    # 성능이 향상되면 모델을 저장합니다\n",
        "    elif train_state['epoch_index'] >= 1:\n",
        "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
        "\n",
        "        # 손실이 나빠지면\n",
        "        if loss_t >= loss_tm1:\n",
        "            # 조기 종료 단계 업데이트\n",
        "            train_state['early_stopping_step'] += 1\n",
        "        # 손실이 감소하면\n",
        "        else:\n",
        "            # 최상의 모델 저장\n",
        "            if loss_t < train_state['early_stopping_best_val']:\n",
        "                torch.save(model.state_dict(), train_state['model_filename'])\n",
        "                train_state['early_stopping_best_val'] = loss_t\n",
        "\n",
        "            # 조기 종료 단계 재설정\n",
        "            train_state['early_stopping_step'] = 0\n",
        "\n",
        "        # 조기 종료 여부 확인\n",
        "        train_state['stop_early'] = \\\n",
        "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
        "\n",
        "    return train_state"
      ],
      "metadata": {
        "id": "PDrJ_YNRXj3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(y_pred, y_target):\n",
        "    _, y_pred_indices = y_pred.max(dim=1)\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "    return n_correct / len(y_pred_indices) * 100"
      ],
      "metadata": {
        "id": "2vI4TmtnXmGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습\n",
        "  - 9분42초 소요"
      ],
      "metadata": {
        "id": "h8iclHhLZ_w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                           mode='min', factor=0.5,\n",
        "                                           patience=1)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm.notebook.tqdm(desc='training routine',\n",
        "                               total=args.num_epochs,\n",
        "                               position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm.notebook.tqdm(desc='split=train',\n",
        "                               total=dataset.get_num_batches(args.batch_size),\n",
        "                               position=1,\n",
        "                               leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm.notebook.tqdm(desc='split=val',\n",
        "                             total=dataset.get_num_batches(args.batch_size),\n",
        "                             position=1,\n",
        "                             leave=True)\n",
        "\n",
        "try:\n",
        "    for epoch_index in range(args.num_epochs):\n",
        "        train_state['epoch_index'] = epoch_index\n",
        "\n",
        "        # 훈련 세트에 대한 순회\n",
        "\n",
        "        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
        "        dataset.set_split('train')\n",
        "        batch_generator = generate_batches(dataset,\n",
        "                                           batch_size=args.batch_size,\n",
        "                                           device=args.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        classifier.train()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # 훈련 과정은 5단계로 이루어집니다\n",
        "\n",
        "            # --------------------------------------\n",
        "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 단계 2. 출력을 계산합니다\n",
        "            y_pred = classifier(x_in=batch_dict['x_data'],\n",
        "                                x_lengths=batch_dict['x_length'])\n",
        "\n",
        "            # 단계 3. 손실을 계산합니다\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "\n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
        "            loss.backward()\n",
        "\n",
        "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
        "            optimizer.step()\n",
        "            # -----------------------------------------\n",
        "\n",
        "            # 정확도를 계산합니다\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            # 진행 상태 막대 업데이트\n",
        "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
        "            train_bar.update()\n",
        "\n",
        "        train_state['train_loss'].append(running_loss)\n",
        "        train_state['train_acc'].append(running_acc)\n",
        "\n",
        "        # 검증 세트에 대한 순회\n",
        "\n",
        "        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
        "        dataset.set_split('val')\n",
        "        batch_generator = generate_batches(dataset,\n",
        "                                           batch_size=args.batch_size,\n",
        "                                           device=args.device)\n",
        "        running_loss = 0.\n",
        "        running_acc = 0.\n",
        "        classifier.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # 단계 1. 출력을 계산합니다\n",
        "            y_pred = classifier(x_in=batch_dict['x_data'],\n",
        "                                x_lengths=batch_dict['x_length'])\n",
        "\n",
        "            # 단계 2. 손실을 계산합니다\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # 단계 3. 정확도를 계산합니다\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
        "            val_bar.update()\n",
        "\n",
        "        train_state['val_loss'].append(running_loss)\n",
        "        train_state['val_acc'].append(running_acc)\n",
        "\n",
        "        train_state = update_train_state(args=args, model=classifier,\n",
        "                                         train_state=train_state)\n",
        "\n",
        "        scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "\n",
        "        if train_state['stop_early']:\n",
        "            break\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"반복 중지\")"
      ],
      "metadata": {
        "id": "_hjvMfYnXn-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset,\n",
        "                                   batch_size=args.batch_size,\n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # 출력을 계산합니다\n",
        "    y_pred =  classifier(batch_dict['x_data'],\n",
        "                         x_lengths=batch_dict['x_length'])\n",
        "\n",
        "    # 손실을 계산합니다\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # 정확도를 계산합니다\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ],
      "metadata": {
        "id": "BWJIM7ErXqaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"테스트 손실: {};\".format(train_state['test_loss']))\n",
        "print(\"테스트 정확도: {}\".format(train_state['test_acc']))"
      ],
      "metadata": {
        "id": "tVcK-4jYXvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.3 추론"
      ],
      "metadata": {
        "id": "oZJceWvNXwKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_nationality(surname, classifier, vectorizer):\n",
        "    vectorized_surname, vec_length = vectorizer.vectorize(surname)\n",
        "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=0)\n",
        "    vec_length = torch.tensor([vec_length], dtype=torch.int64)\n",
        "\n",
        "    result = classifier(vectorized_surname, vec_length, apply_softmax=True)\n",
        "    probability_values, indices = result.max(dim=1)\n",
        "\n",
        "    index = indices.item()\n",
        "    prob_value = probability_values.item()\n",
        "\n",
        "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
        "\n",
        "    return {'nationality': predicted_nationality, 'probability': prob_value, 'surname': surname}"
      ],
      "metadata": {
        "id": "AGymO_kMXypO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# surname = input(\"Enter a surname: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho']:\n",
        "    print(predict_nationality(surname, classifier, vectorizer))"
      ],
      "metadata": {
        "id": "9PsJGTnvX0_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. RNN으로 영화 평점 예측하기**"
      ],
      "metadata": {
        "id": "XxQwhrXHahB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "Q2IVfU-v7ZoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = '/gdrive/My Drive/Colab Notebooks/00_Lectures/NLP/data/'"
      ],
      "metadata": {
        "id": "w0rWCFsy7ZYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBLXbt4fZYFW"
      },
      "source": [
        "### 3.1 패키지 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "lJ8tPcOqZYFa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtcP0mxlZYFd"
      },
      "source": [
        "### 3.2 데이터 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "UHo2afTOZYFe"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = ROOT_PATH+ 'nsmc/'\n",
        "TRAIN_INPUT_DATA = 'nsmc_train_input.npy'\n",
        "TRAIN_LABEL_DATA = 'nsmc_train_label.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA1nbYjKZYFh"
      },
      "outputs": [],
      "source": [
        "train_input = np.load(open(DATA_PATH + TRAIN_INPUT_DATA, 'rb'))\n",
        "train_label = np.load(open(DATA_PATH + TRAIN_LABEL_DATA, 'rb'))\n",
        "prepro_configs = json.load(open(DATA_PATH + DATA_CONFIGS, 'r'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5kqnSe8ZYFh"
      },
      "source": [
        "### 3.3 모델 하이퍼파라메터 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "K6Hh9PSGZYFi"
      },
      "outputs": [],
      "source": [
        "model_name = 'rnn_classifier_en'\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 5\n",
        "VALID_SPLIT = 0.1\n",
        "MAX_LEN = train_input.shape[1]\n",
        "\n",
        "kargs = {'model_name': model_name,\n",
        "        'vocab_size': prepro_configs['vocab_size'],\n",
        "        'embedding_dimension': 100,\n",
        "        'dropout_rate': 0.2,\n",
        "        'lstm_dimension': 150,\n",
        "        'dense_dimension': 150,\n",
        "        'output_dimension':1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDhIDm6lZYFi"
      },
      "source": [
        "### 3.4 RNN 문장 분류기 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 정의"
      ],
      "metadata": {
        "id": "N_Lyy_eD5udN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "HBuZgy8eZYFi"
      },
      "outputs": [],
      "source": [
        "class RNNClassifier(tf.keras.Model):\n",
        "    def __init__(self, **kargs):\n",
        "        super(RNNClassifier, self).__init__(name=kargs['model_name'])\n",
        "        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'],\n",
        "                                     output_dim=kargs['embedding_dimension'])\n",
        "        self.lstm_1_layer = tf.keras.layers.LSTM(kargs['lstm_dimension'], return_sequences=True)\n",
        "        self.lstm_2_layer = tf.keras.layers.LSTM(kargs['lstm_dimension'])\n",
        "        self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
        "        self.fc1 = layers.Dense(units=kargs['dense_dimension'],\n",
        "                           activation=tf.keras.activations.tanh)\n",
        "        self.fc2 = layers.Dense(units=kargs['output_dimension'],\n",
        "                           activation=tf.keras.activations.sigmoid)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.lstm_1_layer(x)\n",
        "        x = self.lstm_2_layer(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 컴파일"
      ],
      "metadata": {
        "id": "HACEco1y55Xl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUSfy3C-ZYFj"
      },
      "outputs": [],
      "source": [
        "model = RNNClassifier(**kargs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b2rDgVRZYFj"
      },
      "source": [
        "- Callback 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "HiEtkFuhZYFj"
      },
      "outputs": [],
      "source": [
        "# overfitting을 막기 위한 ealrystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=1)\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\n",
        "\n",
        "checkpoint_path = DATA_PATH + model_name + '/weights.h5'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "\n",
        "\n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWwu4d06ZYFk"
      },
      "source": [
        "### 3.5 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJfOQ6riZYFl"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
        "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWlNHB7jZYFl"
      },
      "source": [
        "### 3.6 학습 결과"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRQOdMrBZYFc"
      },
      "source": [
        "- 시각화 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "CR1gARo3ZYFd"
      },
      "outputs": [],
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRcTt0R2ZYFe"
      },
      "source": [
        "- 랜덤 시드 고정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKzVS8qSZYFf"
      },
      "outputs": [],
      "source": [
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정확도 그래프"
      ],
      "metadata": {
        "id": "q8-bhUSV9O3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "YDB44aHLZYFl"
      },
      "outputs": [],
      "source": [
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 손실 그래프"
      ],
      "metadata": {
        "id": "6Uw_Itrf9Spj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "wjNaxVn3ZYFl"
      },
      "outputs": [],
      "source": [
        "plot_graphs(history, 'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8jy8iHXZYFm"
      },
      "source": [
        "### 3.7 학습된 모델 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 테스트 데이터 불러오기"
      ],
      "metadata": {
        "id": "Y0k9TdJ79dWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "n0eSELdJZYFm"
      },
      "outputs": [],
      "source": [
        "TEST_INPUT_DATA = 'nsmc_test_input.npy'\n",
        "TEST_ID_DATA = 'nsmc_test_id.npy'\n",
        "\n",
        "test_input = np.load(open(DATA_PATH + TEST_INPUT_DATA, 'rb'))\n",
        "test_input = pad_sequences(test_input, maxlen=test_input.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxCOKylCZYFm"
      },
      "source": [
        "- 베스트 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coiwuOLKZYFm"
      },
      "outputs": [],
      "source": [
        "SAVE_FILE_NM = 'weights.h5'\n",
        "\n",
        "model.load_weights(os.path.join(DATA_PATH, model_name, SAVE_FILE_NM))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IcWdI0xZYFm"
      },
      "source": [
        "- 테스트 데이터 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq23LioGZYFn"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_input, batch_size=BATCH_SIZE)\n",
        "predictions = predictions.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "jxNSoG_t-uFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZCtBKZV9ymt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}