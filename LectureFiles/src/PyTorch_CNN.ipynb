{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PyTorch를 이용한 CNN**"
      ],
      "metadata": {
        "id": "-EXml_gbfb8Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMAadJ2yhHwW"
      },
      "source": [
        "## **1. CNN으로 Fashion MNIST 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 패키지 불러오기"
      ],
      "metadata": {
        "id": "Yd-NDW1Mf6DR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG-KIKEVhHwO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. 기본 설정"
      ],
      "metadata": {
        "id": "SHmuej3SgKhX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSwq8JNVhHwQ"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo8a4q1PhHwS"
      },
      "source": [
        "EPOCHS     = 40\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKlMNFZghHwT"
      },
      "source": [
        "### 1-3. 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf1IV-XehHwV"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./.data',\n",
        "                   train=True,\n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./.data',\n",
        "                   train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxQThhmxhHwX"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ0FKNeuhHwZ"
      },
      "source": [
        "- 하이퍼파라미터\n",
        "\n",
        "  - to() 함수는 모델의 파라미터들을 지정한 곳으로 보내는 역할\n",
        "  - 일반적으로 CPU 1개만 사용할 경우 필요는 없지만, GPU를 사용하고자 하는 경우 to(\"cuda\")로 지정하여 GPU로 보내야 함.\n",
        "  - 지정하지 않을 경우 계속 CPU에 남아 있게 되며 빠른 훈련의 이점을 누릴 수 없음\n",
        "  - 최적화 알고리즘으로 파이토치에 내장되어 있는 optim.SGD 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbXgnWsJhHwb"
      },
      "source": [
        "model     = Net().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CjS1N-xhHwb"
      },
      "source": [
        "### 1-4. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E3kdoBGhHwc"
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 200 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4p9ArohHwd"
      },
      "source": [
        "### 1-5. 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fAw0O1KhHwd"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            # 배치 오차를 합산\n",
        "            test_loss += F.cross_entropy(output, target,\n",
        "                                         reduction='sum').item()\n",
        "\n",
        "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk0N__aVhHwe"
      },
      "source": [
        "### 1-6. 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wml-eWSUhHwf"
      },
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
        "          epoch, test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QlmHdnNhMs2"
      },
      "source": [
        "## **2. 좀더 깊은 CNN 모델 적용**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. 패키지 불러오기"
      ],
      "metadata": {
        "id": "ZqlFwoekhouZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIcVUsoAhMs3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. 기본 설정"
      ],
      "metadata": {
        "id": "5g7ytyXMhtf4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9JELTZhMs5"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-3. 데이터 준비"
      ],
      "metadata": {
        "id": "GkqcTf1ghySy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtBBerdAhMs5"
      },
      "source": [
        "- 하이퍼파라미터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_rqgP6rhMs6"
      },
      "source": [
        "EPOCHS     = 300\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDondY7lhMs6"
      },
      "source": [
        "- 데이터셋, 데이터 로더 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noyZVo4OhMs7"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('./.data',\n",
        "                   train=True,\n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.RandomCrop(32, padding=4),\n",
        "                       transforms.RandomHorizontalFlip(),\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                            (0.5, 0.5, 0.5))])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('./.data',\n",
        "                   train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                            (0.5, 0.5, 0.5))])),\n",
        "    batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t7TQe_9hMs9"
      },
      "source": [
        "### 2-4. ResNet 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5psL_7ZhMs9"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpyKV_fdhMs_"
      },
      "source": [
        "model = ResNet().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=0.0005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRCdF4DLhMtA"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0rQ60OIhMtA"
      },
      "source": [
        "### 2-5. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIQzBKGzhMtB"
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZaArJEahMtB"
      },
      "source": [
        "### 2-6. 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_BSoc1NhMtC"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            # 배치 오차를 합산\n",
        "            test_loss += F.cross_entropy(output, target,\n",
        "                                         reduction='sum').item()\n",
        "\n",
        "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzD6J6BlhMtC"
      },
      "source": [
        "### 2-7. 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73-qQT7qhMtC"
      },
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    scheduler.step()\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
        "          epoch, test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}